% Change focus: How to process stock prices. How to correlate them. Prediction only as information source
% Dont change too much but update the overall structure

% Feedback:
% review paragraphs - is it too big compared to its importance
% Break down approaches if not that related
% classify if approaches are necessary/important
% would a table help the thesis structure?


In the research area of modelling economic variables a plethora of approaches have been proposed, utilizing econometrics, Natural Language Processing (NLP) and machine learning. Predicting a stock's future behaviour is among the main research topics. Due to its practical application and its erroneous promise of easily earned riches, it received great attention. But the stock price prediction is a controversial topic and lacks a good theoretical foundation which justifies the approach of forecasts on finance markets. By ignoring these issues and applying machine learning approaches in sandbox like environments, some studies do not fully reflect the problem space and therefore erroneously promise investors easily earned riches. Consequently, the following related work is selected in terms of theoretical background required for understanding stock prices in general and promising prediction models on similar data. For a more comprehensive review of related work, I recommend the work by \citet{Hsu2016BridgingEconomists, KhadjehNassirtoussi2014TextReview, Cavalcante2016ComputationalDirections}.


A major issue in the related work of financial predictions is the comparability of results. There are a lot of different evaluation techniques like error rate \cite{Peng2016LeverageNetworks, Yoshihara2014PredictingNetworks}, Root Mean Squared Error \cite{Feuerriegel2018Long-termDisclosures}, Mean Absolute Percentage Error \cite{Bollen2011TwitterMarket, Xiong2015DeepTrends}, Theil’s inequality coefficient \cite{Bao2017AMemory.}, Cohen's kappa \cite{Tsantekidis2017UsingMarkets}, Area Under Curve \cite{Ballings2015EvaluatingPrediction} and Matthews Correlation Coefficient (MCC) \cite{Ding2014UsingInvestigation}. Some studies do not report an established data prediction metric at all, but instead test their model integrated in a trading strategy \cite{Deng2017DeepTrading, Ruiz2012CorrelatingActivity}. Of course, the real-world application is of high interest and should be tackled in the evaluation, but in addition, one should report some basic metrics. Consequentially, many scientists evaluate their performance in comparison to the probability by chance \cite{KhadjehNassirtoussi2014TextReview}. Another issue is the selection of the financial market since its maturity has a significant effect on the markets predictability \cite{Hsu2016BridgingEconomists}. For those reasons, the related work is not compared by their reported evaluation metrics but evidence is collected, on how to improve text-based stock price predictions in general.

With the high demand for good prediction models a vast number of data is available to researches. On the one hand, some studies focus on predicting the trend of stock prices via technical indicators like Bollinger Bands, momentum or Moving Average Convergence Divergence (MACD) \cite{Lee2014OnPrediction, Li2014NewsAnalysis, Yoshihara2014PredictingNetworks, Akita2016DeepInformation}, Bao et al. (2017), Patel et al. (2015). On the other hand, the whole market behaviour is predicted by the stock index or the volatility index \cite{Yang2002SupportPrediction, Bollen2011TwitterMarket, Zhang2011PredictingFear, Ding2014UsingInvestigation, Xiong2015DeepTrends}. Because the correlation of stock prices will be investigated, this work focuses on separate stocks instead of technical indicators or market indices. Still, these studies are of interest because they have to tackle similar problems caused by the noisy and stochastic nature of stock markets. 

% The overall field of market analysis is a very interdisciplinary one for which various research directions might be of interest.
The following approaches are splitted into two subsection. Because correlating stock prices requires conducting an intensive regression analysis, the following subsection particularly focuses on the predictability of time series in the context of econometrics and statistics. Methods like cross-correlation, mutual information and Granger Causality are proposed to measure and compare financial time series. Even though econometrics appear to be a hot topic in modern society, some fundamental knowledge of this research area is more than a century old and gives insights on how to set up meaningful similarity measures and models for noisy economic time series. Subsequently, the second subsection covers mixed prediction models for stock prices incorporating external information sources. Latest approaches benefit from the availability of data and computational power and therefore are better able to incorporate unstructured data like news and social media. To tackle the big amount of unstructured data, established methods from natural language processing and machine learning are applied.

\subsection{Statistical Analysis}

% \todo{Add Mantegna arguing for cross-dependencies in stock prices}
% Mantegna recalls the basic assumption of portfolio selection theory that there exists some kind of cross-dependencies between different stock prices.
% \emph{The stock market is far more complex than a collection of several independent random walks} (Mantegna)
% correlated, uncorrelated or weakly anti-correlated between pairs of stock prices on the NYSE (Mantegna)  \cit€{R.N.MANTEGNACross-correlationMarkets}

The predictability of the stock market is a highly competitive research area for which a great diversity of approaches have been proposed. On the one hand, scientists with economical or statistical background investigate the characteristics and quality of economic variables. On the other hand, scientists from the field of machine learning describe and evaluate new data-driven prediction models, often without tackling the systematic errors present in raw economic data. That is because machine learning models are supposed to learn relevant relationships and adapt their weights to the underlying probability distributions of the features. Bad or good prediction results are mostly used as arguments for the predictive performance of a model. Yet, the selection of the dataset itself might play a crucial role in the feasibility of a classification or regression problem. For example, \citet{Sun2016TradePrediction} report 70~\% accuracy for their matrix factorization model during training but only 51~\% on test data which is not a significant improvement compared to random guessing. On the contrary, \citet{Lee2014OnPrediction} present a Random Forest Classifier for predicting separate stock prices with an accuracy 22.2~\% higher than random guessing. Although results like these sound promising, it could be the consequence of an unsafe evaluation. Among various factors influencing an experiments outcome, models without cross-validation are prone to the \enquote{lucky sample effect}, as demonstrated by \citet{Hsu2016BridgingEconomists}. The time series might be lacking ergodic properties and therefore report erroneously high accuracy.

Further, \citet{Hsu2016BridgingEconomists} examine the crossfertilization between machine learning and econometrics in detail by shedding light on the applicability of the EMH and the disagreement between machine learning and financial economics literature. They collect and test several hypotheses concluding with some interesting findings: \begin{enumerate*}[label=(\roman*)] \item Technical indicators do not add a significant improvement to predictions;
\item SVM outperforms ANNs by a large difference;
\item machine learning approaches outperform econometric models (e.g. autoregression).
\end{enumerate*}

\citet{Kim2016AModel} applies a rule-based classifier on stocks from the energy sector of the US stock market. He calculates the cross-correlation among pairs of stocks and predicts a stocks price direction by considering the lagged stock price of another highly cross-correlated stock. For selected pairs of stocks they report an accuracy of 87.2~\% in average. Even though they select only highly cross-correlating pairs, potential spurious correlation are not considered which might arise due to unfiltered autocorrelations. As already pointed out by \citet{Granger1974SpuriousEconometrics} several decades ago, mis-specifications like omitted variables, irrelevant variables included or autocorrelations can lead to spurious correlation/regression. Therefore, instantaneously observed correlations might diminish over time.

\citet{Ruiz2012CorrelatingActivity} exploit these auto-dependencies of stock prices by training Auto Regression and Vector Auto Regression models with the Ordinary Least Squares (OLS) method to predict the final price on each day in the context of a regression task. Their approach combines stock price data with numerical features extracted from Twitter posts, e .g. the number of forwardings for a post.  Unfortunately, they do not inspect their data for homoscedasticity which is one of the requirements of the Gauss-Markov theorem for OLS to be the best linear unbiased estimator \cite{Hallin2014Gauss-MarkovStatistics}. Furthermore, they do not report statistical measures which can be compared to other work, but test their approach in a daily trading simulation ending up with 0.32~\% gain for the best model.

Instead of proving the predictability of a new introduced features by feeding it into a prediction model, \citet{Vlastakis2012InformationVolatility} apply a regression analysis between the demand for market-related information and market variables like volatility and stock prices. The overall information demand is represented by Google's Search Volume Index for the search keyword S\&P 500. The analysis includes hypothesis tests for Pearson's $r$ and uni- or bidirectional Granger Causality between the selected variables. In conclusion, they show that these relationship exists with a high certainty.

\citet{Kosapattarapim2017GrangerThailand} presents a very detailed procedure on inspecting Granger Causality between the stock exchange index of Thailand and the exchange rate between Thai Baht against US dollars. To ensure unit root time series and a cointegration relationship at a 5~\% significance level, they apply the Augmented Dickey-Fuller (ADF) \cite{Dickey1976IntroductionSeries} and the Johansen test. Both properties are mandatory before one can look for actual Granger Causality. Their results indicate an unidirectional causality from stock prices to exchange rate.

Besides univariate linear measures like autocorrelation and volatility another nonlinear property of time-series is the complexity/regularity which can be measured by approximate or sample entropy. The common Shannon entropy is not applicable since stock prices are considered to be non-stationary time-series. Therefore, \citet{Pincus2004IrregularitySeries} apply the approximate entropy on stock prices to measure a system's stability or irregularity. They conclude that this irregularity is a complementary property to volatility which is paramount for meaningful financial data analysis.

For a more conscious inspection of nonlinear auto-dependencies in finance time series, \citet{Dionisio2004MutualSeries} compare the normalized mutual information with Pearson's $r$ for several stock price indices. Referring to related work, they recall the assumption of a strong relationship between entropy, dependence and predictability. To exclude the linear auto-dependencies, they filter the data by taking the residuals of an autoregressive–moving-average process. Unlike the linear correlation, mutual information still indicates a significant dependency on the residuals without any foreknowledge on the theoretical probability distribution or the type of dependency.

% Detrended Cross-Correlation, Transfer Entropy, Sample Entropy between stock markets (2013)
% https://link.springer.com/article/10.1007/s11071-012-0680-z

% No Contagion, Only Interdependence: Measuring Stock Market Co-Movements (Forbes and Rigoborn, 1999)
% https://www.nber.org/papers/w7267.pdf


\subsection{Prediction Models}

Though this work is not focussing on predicting stock prices, the related work in this research area reveal relevant insights on the data and feature extraction. 

In the last decades the study of predictability on the stock market was a very popular topic for scientists. Especially the rapid achievements in predicting stock prices by applying models from the area of representation learning attracted attention. The first application of Neural Networks (NN) 30 years ago initiated a run on applying NNs and other related models. Before this time there were already other learning algorithms present like Linear Regression (LR), Support Vector Machines (SVM) and Genetic Algorithms. Usually, approaches based on NNs outperformed those other existing techniques partly by far \cite{Hsu2016BridgingEconomists, Yoo2005MachineEvaluation}.

To come up with valuable input for prediction models, the unstructured data needs to be preprocessed. This process has a significant impact on the success of the prediction models and therefore requires sophisticated approaches. \citet{KhadjehNassirtoussi2014TextReview} provide a broad overview of related work based on these three aspects and review in terms of feature-selection, dimensionality-reduction and feature-representation.

% For instance, \citet{Tsantekidis2017UsingMarkets} apply a LSTM on Limit Order Books containing all buy and sell orders of a stock for prediction a stock prices movement. 
% One of the first NLP approaches on price prediction: https://www.researchgate.net/publication/3776378_Daily_stock_market_forecast_from_textual_web_data



% \suggest{To tackle the big datasets deep learning technique are applied as an improvement of previous NNs. As pointed out by \citet{Chong2017DeepStudies} deep neural networks (DNN) are able to extract abstract features from a large dataset without any prior knowledge of predictors.}{}
Since external information related to stock markets is not directly observable, meaningful proxies are extracted from unstructured data like forum posts, news, social media and SEC filings. The most popular text sources are financial news \cite{Peng2016LeverageNetworks, KhadjehNassirtoussi2015TextSentiment, Zhai2007CombiningPrediction} because they are expected to represent new events influencing the financial market. Their importance is mostly reflected within the stock prices of the directly following days and loses its meaning over a longer time period \cite{Ding2014UsingInvestigation}. Therefore, many scientists incorporate news for short-time prediction models \cite{KhadjehNassirtoussi2014TextReview}. They consider the news and the previous daily stock prices to predict the intraday price movement for the next day. The market return for this relatively short time span is known as \enquote{open-to-close}. Various methods for extracting abstract representations of news have been proposed over the last years. Most of the following presented approaches can be counted to deep learning since they rely on big datasets and learn abstract hierarchical representations by applying two or more hidden layers in their networks.

\citet{Ding2014UsingInvestigation} compares linear and nonlinear approaches for prediction using events-based document representations without incorporating historical stock price data. They extract events by applying Open Information Extraction (OIE) on news articles from \textit{Reuters} and \textit{Bloomberg}. Thereby each article is transformed into a tuple of subject, predicate verb and object. The applied DNN achieves 60~\% accuracy for predictions of the S\&P 500 index. For the prediction of individual stocks it even surpasses 70~\% accuracy, even though they do not consider the stock price of the previous days at all. One main finding of their work is the advantage of less but more qualitative data (news titles) in contrast to much noisy text data (whole article contents). This indicates that thoughtful preprocessing is a very import task for text-based approaches. \citet{Zhai2007CombiningPrediction} suggests a SVM for combining textual and numerical data. News texts from \textit{The Australian Financial Review} are accumulated by a semantic network called WordNet. It creates 30-dimensional semantic representation of an article's content. Compared with their own single source systems, results of 70~\% accuracy indicate that news combined with technical indicators can improve the predictability of a stock's daily movements. Since there are events on the finance market whose impact do not fade out within days but weeks or months, \citet{Yoshihara2014PredictingNetworks} sheds light on a novel deep network for incorporating events with longer-term impacts. They enhance a Recurrent Neural Networks-Restricted Boltzmann Machine (RNN-RBM) with a Deep Belief Network (DBN) and apply it for prediction of Nikkei stock price movements on the next day. However the results are not tremendously above their heuristic baseline (about 3~\% improvement), indicating that their approach only adds a slight improvement. Hence, their model might lack some contextual information since it only relies on the news articles collected from \textit{Nikkei Newspaper}, not considering the previous behaviour of the related stock price.

Previous work usually only considers one company for predicting its future stock price as pointed out by \citet{Akita2016DeepInformation}. Instead, they feed related articles and historical prices of ten companies within the same industry at once into a LSTM to predict the close prices of all ten companies by regression analysis. \textit{Nikkei newspapers} are preprocessed using Paragraph Vectors which learns fixed-length feature vectors from variable-length texts. Their market simulation indicates that incorporating multiple companies from the same industry is very effective for stock price prediction. Unfortunately, a comparable evaluation using metrics like accuracy is missing.

A recent approach by \citet{Chen2018IncorporatingPrediction} explores the setup and application of a corporation graph for a prediction model. Due to its relevance this approach will be examined more detailed as the previous ones. A graph is proposed which contains nodes representing stock companies from Shanghai Stock Exchange and Shenzhen Stock Exchange and their shareholders. The weighted edges between those nodes indicate the shareholding ratio. Based on this graph two approaches are presented for incorporating the data into a NN: The first one maps each company node into a vector using various node embedding techniques. For predicting a company's stock price movement the ten closest nodes are selected, the related historical prices accumulated into a vector and fed into a LSTM-Encoder. Based on the outcome of this layer they apply a binary classification layer which should output if the stock price will rise or fall on the next day (open-to-close intraday movement). The second more advanced approach tackles the issue of summarizing all the information and not considering each relation separately. Therefore, they apply a joint model based on a Graph Convolutional Network (GCN) which incorporates the whole network. A combination of a LSTM baseline and their GCN model yields the best results. While the baseline obtains an accuracy of about 53~\% the graph based enhancement improves it by almost 5~\%. 

Both of the introduced approaches sound promising for investigating them closer in my work.
The latter work can be seen as a proof of concept. Considering relations among companies can improve the predictability of a stocks behaviour. However, they select companies by their similarity since their graph's edges only represent share ownerships. For the well-being of a company, and therefore also of its stock, the performance of cooperating companies is crucial. If a supplying or subsidiary company is not doing well, the graph should reflect which companies might be infected by this one. Therefore, it is suggested to extract representative features of business relations among companies like the same industry, subsidiaries, co-operations and supplies.

% \input{figures/research_overview}
% \todo{Deprecated from expose}
% For a quick overview Table~\ref{table:research_overview} provides a summary of the following presented recent work.


% Authors (year)

% Features (e.g. Technical Indicators, Historic Closing Stock Prices, Historic OHLCV, Word Vector, Sentiment Vector, Bag of Words/Keywords, Fundamental Var.)
% Prediction Model / Method (e.g. ANN, LSTM, RL, VAR, Cross-Correlation, SVM, LR, SOFNN, DBN)
% Predicted Variable (Regression, Binary, 3 Classes, Other (e.g. 5 classes))
%? Prediction Window (Short-/Mid-/Long-Term)
% Performance measure (e.g. MSE/RMSE, Cohens Kappa, F-Score, Accuracy, Return of Invest)

% Data source (provider)
% Type of data (Stock Price, Finance News, Stock Index, US-Dollar-Index, Tweets Sentiment, Limit Order Books)
% Amount of Samples (train/val/test?)
% Market / Stock Exchange (e.g. Bombay Exchange, NYSE, NASDAQ, SSZE)
% Sampling period (covered years)